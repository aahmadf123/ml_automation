```markdown
# ML Automation & Lossâ€‘History Pipeline

A unified MLOps project that powers a daily homeowner lossâ€‘history pipeline with:

- **Data ingestion & preprocessing** via Apache Airflow on EC2
- **Drift monitoring & selfâ€‘healing** with versioned reference means
- **Automated hyperparameter tuning & training** using HyperOpt + XGBoost
- **Model tracking & promotion** in MLflow (with metrics, SHAP + Actualâ€‘vsâ€‘Pred plots)
- **Realâ€‘time dashboard** (Next.js + WebSockets) for system and model observability
- **Notifications & alerts** via Slack integration
- **Persistent storage** of data, artifacts, and models on S3

---

## ğŸš€ Features

1. **Endâ€‘toâ€‘end pipeline**
   - Ingest raw CSV from S3 â†’ preprocess (missing data, outliers, encoding) â†’ parquet
   - Validate schema with Pandera â†’ version & snapshot on S3
2. **Drift detection & selfâ€‘healing**
   - Compute & upload timestamped reference means
   - Compare new data â†’ branch to selfâ€‘heal or train
3. **Automated training & tuning**
   - HyperOpt search for best XGBoost hyperparams
   - Fallback from TimeSeriesSplit â†’ random split
   - Log RMSE, MSE, MAE, RÂ² to MLflow
   - Generate + upload SHAP summary and Actual vs Predicted plots
   - Autoâ€‘promote to â€œProductionâ€ stage in MLflow Registry
4. **Dashboard & API**
   - Next.js frontend served on Vercel (or EC2)
   - WebSocket updates for live metrics & drift alerts
5. **Alerts & notifications**
   - Slack webhooks for drift, profiling summaries, training results
6. **Version control & collaboration**
   - All DAGs, scripts, and dashboard code in a single GitHub repo
   - Environment variables isolated in `.env`

---

## ğŸ“ Architecture

```

```
                            +------------+      +----------------+
```

CSV on S3 â”€â”€â”€â–¶ Ingest Task  â”€â”€â–¶ Parquet â”€â–¶ Preprocess Task â”€â”€â–¶ Processed Parquet
(Airflow)                       â”‚
â”‚
â–¼
Drift Check Task
â”€â”€â–¶ Selfâ€‘heal â”€â–¶ Slack
â”€â”€â–¶ Train Branch â”€â”€â”€â–º HyperOpt + XGBoost
â”‚
â–¼
MLflow (Metrics + Models)
â”‚
â–¼
Next.js Dashboard â† WebSocket API â† Lambda / AppSync
â”‚
â–¼
Slack Notifications

````

---

## ğŸ› ï¸ Getting Started

### Prerequisites

- **AWS account** with S3 bucket & IAM credentials
- **EC2 instance** (Ubuntu) with Docker (or Python 3.12 venv)
- **Apache Airflow** (2.10+) installed on EC2
- **MLflow** server running (can be on EC2 or separate)
- **Next.js** for dashboard (local or Vercel)

### Configuration

1. **Clone repository**
   ```bash
   git clone git@github.com:YourOrg/ml_automation.git
   cd ml_automation
````

2.  **Create & populate `.env`** (see [`.env.example`](https://www.google.com/search?q=./.env.example))
3.  **Install Python dependencies**
    ```bash
    cd dags
    pip install -r requirements.txt
    ```
4.  **Initialize Airflow DB & start services**
    ```bash
    export AIRFLOW_HOME=~/airflow
    airflow db init
    airflow webserver --port 8080 &
    airflow scheduler &
    ```
5.  **Start MLflow server**
    ```bash
    mlflow server \
      --backend-store-uri sqlite:///mlflow.db \
      --default-artifact-root s3://$S3_BUCKET/mlruns \
      --host 0.0.0.0 --port 5000
    ```
6.  **Deploy Dashboard**
      - Locally: `cd loss-history-dashboard && npm install && npm run dev`
      - Vercel: Connect to `main` branch & set same env vars

-----

## ğŸ“‚ Repository Structure

```
ml_automation/
â”œâ”€â”€ dags/                         # Airflow DAGs & task modules
â”‚   â”œâ”€â”€ homeowner_dag.py
â”‚   â””â”€â”€ tasks/                    # Ingestion, preprocessing, drift, training, etc.
â”œâ”€â”€ loss-history-dashboard/       # Next.js frontend & WebSocket client
â”œâ”€â”€ mlflow-export-import/          # Optional scripts for MLflow model registry
â”œâ”€â”€ .env.example                  # Example env vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ airflow.cfg                   # Airflow configuration
â”œâ”€â”€ webserver_config.py           # Airflow webserver settings
â””â”€â”€ README.md                     # Project overview & instructions
```

-----

## ğŸ“¦ Usage

1.  **Trigger the pipeline**
      - Auto: runs daily at 10 AM via schedule
      - Manual:
        ```bash
        airflow dags trigger homeowner_loss_history_full_pipeline
        ```
2.  **Watch training & promotion**
      - Check MLflow UI: `http://<mlflow-host>:5000`
3.  **View Dashboard**
      - Visit `http://localhost:3000` or your Vercel URL
4.  **Inspect Alerts**
      - Slack channel: `#alerts`

-----

## ğŸ¤ Contributing

1.  Fork & branch off `main`
2.  Add or update DAGs, tasks, or dashboard components
3.  Update `.env.example` if you introduce new env vars
4.  Submit a PR, and review CI checks

